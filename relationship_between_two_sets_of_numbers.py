# -*- coding: utf-8 -*-
"""Relationship between two sets of numbers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L7VJakbSvDQ3vMYV19NP-A72QGCwsrkH
"""

import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras import regularizers
from matplotlib import pyplot as plt

# Provide the number data.
# Consider the following sets of numbers. Can you see the relationship between them?
# Y=3X+1
xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)
ys = np.array([-2.0, 1.0, 4.0, 7.0, 10.0, 13.0, 16.0], dtype=float)

# This callback will stop the training when there is no improvement in
# the loss for 10 consecutive epochs.
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)

# Define and compile the neural network
model = tf.keras.Sequential(
    [
        tf.keras.Input(shape=(1,)),
        Dense(units=25, activation='relu', kernel_regularizer='l1_l2', name='Layer1'),
        Dense(units=15, activation='relu', kernel_regularizer='l1_l2', name='Layer3'),
        Dense(units=10, activation='relu', kernel_regularizer='l1_l2', name='Layer4'),
        Dense(units=1, activation='linear', name='Layer5')
    ], name="relationship"
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
    loss='mean_squared_error')

model.summary()

# Train the neural network
history = model.fit(xs, ys, epochs=5000, batch_size=128, callbacks=[callback], verbose=1)

print(f"Only {len(history.history['loss'])} epochs ran.")

model.summary()

plt.plot(history.epoch, history.history['loss'], label='total loss')
plt.title('Training Loss')
plt.xlabel('Number of Epochs')
plt.ylabel('Mean squared error (MSD)')
plt.legend()
plt.show()

# Use the model on unseen number data
print(model.predict([5.0]))

print(model.predict([3.0]))